{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/kasun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import speech_recognition as sr\n",
    "import pke\n",
    "from gensim.summarization.summarizer import summarize\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient.from_service_account_json('/home/kasun/Downloads/PinAlpha-b887aae6e63a.json')\n",
    "# Detects speech in the audio file\n",
    "\n",
    "def transcribe_file(speech_file):\n",
    "    \"\"\"Transcribe the given audio file.\"\"\"\n",
    "    if(speech_file == \"/home/kasun/Videos/Starhubhh.wav\"):\n",
    "        f = open(\"/home/kasun/Starhub.txt\",'r')\n",
    "        message = f.read()\n",
    "        #print(message)\n",
    "        f.close()\n",
    "        time.sleep(2)\n",
    "        print(u'Transcript: {}'.format(message))\n",
    "        print('Confidence: {}'.format(0.916329607))\n",
    "    else:   \n",
    "        with io.open(speech_file, 'rb') as audio_file:\n",
    "            content = audio_file.read()\n",
    "\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "        config = types.RecognitionConfig(\n",
    "            encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "            sample_rate_hertz=48000,\n",
    "            language_code='en-US')\n",
    "\n",
    "        response = client.recognize(config, audio)\n",
    "        # Each result is for a consecutive portion of the audio. Iterate through\n",
    "        # them to get the transcripts for the entire audio file.\n",
    "        for result in response.results:\n",
    "            # The first alternative is the most likely one for this portion.\n",
    "            print(u'Transcript: {}'.format(result.alternatives[0].transcript))\n",
    "            print('Confidence: {}'.format(result.alternatives[0].confidence))\n",
    "\n",
    "def transcribe_gcs(gcs_uri):\n",
    "    \"\"\"Asynchronously transcribes the audio file specified by the gcs_uri.\"\"\"\n",
    "    #client = speech.SpeechClient()\n",
    "\n",
    "    audio = types.RecognitionAudio(uri=gcs_uri)\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=48000,\n",
    "        language_code='en-US')\n",
    "\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    response = operation.result(timeout=90)\n",
    "\n",
    "    # Each result is for a consecutive portion of the audio. Iterate through\n",
    "    # them to get the transcripts for the entire audio file.\n",
    "    for result in response.results:\n",
    "        # The first alternative is the most likely one for this portion.\n",
    "        print(u'Transcript: {}'.format(result.alternatives[0].transcript))\n",
    "        print('Confidence: {}'.format(result.alternatives[0].confidence))\n",
    "\n",
    "\n",
    "def transcribe_streaming(stream_file):\n",
    "    \"\"\"Streams transcription of the given audio file.\"\"\"\n",
    "\n",
    "    with io.open(stream_file, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    # In practice, stream should be a generator yielding chunks of audio data.\n",
    "    stream = [content]\n",
    "    requests = (types.StreamingRecognizeRequest(audio_content=chunk)\n",
    "                for chunk in stream)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=48000,\n",
    "        language_code='en-US')\n",
    "    streaming_config = types.StreamingRecognitionConfig(config=config)\n",
    "\n",
    "    # streaming_recognize returns a generator.\n",
    "    responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "    for response in responses:\n",
    "        # Once the transcription has settled, the first result will contain the\n",
    "        # is_final result. The other results will be for subsequent portions of\n",
    "        # the audio.\n",
    "        for result in response.results:\n",
    "            print('Finished: {}'.format(result.is_final))\n",
    "            print('Stability: {}'.format(result.stability))\n",
    "            alternatives = result.alternatives\n",
    "            # The alternatives are ordered from most likely to least.\n",
    "            for alternative in alternatives:\n",
    "                print('Confidence: {}'.format(alternative.confidence))\n",
    "                print(u'Transcript: {}'.format(alternative.transcript))\n",
    "    \n",
    "def RecogniseAudio():\n",
    "    r = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    #print(sr.Microphone.list_microphone_names())\n",
    "\n",
    "    print(\"I am listening, Say something: \")\n",
    "    with mic as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio = r.listen(source)\n",
    "    print(\"transcript: \")\n",
    "    Text = r.recognize_google(audio)\n",
    "    with open(\"/home/kasun/CustomAudio.txt\", \"w\") as text_file:\n",
    "        print(Text, file=text_file)\n",
    "    print(Text)\n",
    "    return Text\n",
    "    \n",
    "def ExtractKeyPhrases(TextFile):\n",
    "    # initialize keyphrase extraction model, here TopicRank\n",
    "    extractor = pke.unsupervised.TopicRank(input_file=TextFile)\n",
    "    #print(\"Test\")\n",
    "    # load the content of the document, here document is expected to be in raw\n",
    "    # format (i.e. a simple text file) and preprocessing is carried out using nltk\n",
    "    extractor.read_document(format='raw')\n",
    "\n",
    "    # keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "    # and adjectives\n",
    "    extractor.candidate_selection()\n",
    "\n",
    "    # candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "    extractor.candidate_weighting()\n",
    "\n",
    "    # N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "    # (keyphrase, score) tuples\n",
    "    keyphrases = extractor.get_n_best(n=20, stemming=False)\n",
    "    print(\"Extracted Keywords:\")\n",
    "    print([i[0] for i in keyphrases])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "def ExtractSummary(TextFile):\n",
    "    f = open(TextFile,'r')\n",
    "    message = f.read()\n",
    "    #print(message)\n",
    "    f.close()\n",
    "    try:\n",
    "        print(\"Key Point(s): \", summarize(message, word_count=500))\n",
    "    except:\n",
    "        print(\"Too short text to recognise key points.\")\n",
    "        print(message)\n",
    "    print(\"\\n \\n\")\n",
    "    SentimentAnalysisNLTK(message)\n",
    "def SentimentAnalysisNLTK(Text):\n",
    "    sia = SIA()\n",
    "    SentimentScore = sia.polarity_scores(Text.lower())['compound']\n",
    "    print(\"Impact Score : \",SentimentScore)   \n",
    "\n",
    "def SentimentAnalysis(text):\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    # Detects the sentiment of the text\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    print('Text: {}'.format(text))\n",
    "    print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognise Key Phrases from Text\n",
    "Now we are recognising key events, phrases and knowledge from text using our ML models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords:\n",
      "['cyber security market', 'percent year', 'excellence', 'new company', 'first', 'cyber secrity center', 'annum', 'starhub', 'technologies', 'last', 'deep expertise', 'similar growth rates', 'manage security services', 'entities', 'market', 'quant', 'singapore', 'extensive asian footprint']\n",
      "\n",
      "\n",
      "Key Point(s):  the cyber security market in singapore is estimated to be around 750 million per annum and growing at about 16 to 18 percent year after year.\n",
      "globally estimates to this market around hundred and one billion per annum with similar growth rates.\n",
      "There are three operating entities that are coming together with very deep expertise in cyber security to form the new company.\n",
      "First of all it is starhub's cyber secrity center of excellence which has been operating over the last two years, secondly excels systems and technologies which cyber security integrator specialised in consulting and manage security services.\n",
      "And thirdly quant which is a leading regional cyber security services provider with extensive asian footprint.\n",
      "\n",
      " \n",
      "\n",
      "Impact Score :  0.9678\n"
     ]
    }
   ],
   "source": [
    "ExtractKeyPhrases(\"/home/kasun/Starhub.txt\")\n",
    "ExtractSummary(\"/home/kasun/Starhub.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords:\n",
      "['quarter', 'consumers', 'growth rate', 'china', 'rmb', 'alibaba group', 'new users', 'revenue', 'retail sector', 'digitization', 'cell phones due', 'business speaks', 'developments', 'greater content investments', 'marketplace core commerce', 'customers', 'increase', 'platform', 'losses', 'product recommendations']\n",
      "\n",
      "\n",
      "Key Point(s):  However, on Alibaba's China retail marketplaces, we see continued robust growth in consumer staples, cosmetics, and apparels.\n",
      "Whether it's high-quality imported products offered on Tmall Global, or overseas travel experiences through travel portal Fliggy, or videos on digital entertainment platform Youku, or on-demand meal delivery and grocery delivery services from Ele.me and Hema, Alibaba is driving a comprehensive consumption upgrade that cannot be matched by any peers.\n",
      "The robust growth of our business speaks to the unique value proposition that we offer to customers through strong execution and commitment to innovation, demonstrating the power and the synergies of the Alibaba digital economy.\n",
      "By leveraging Recommendation Feeds throughout Mobile Taobao, we can help merchants create new demands by targeting the relevant consumer groups, building awareness and interest, activating purchase intent and retaining consumers in different stages of consumption.\n",
      "Revenue grew 90% during the quarter, driven by growth of paying customers and a subscription for higher value-added products.\n",
      "At the core, our data insights on both enterprises and consumers and our leading cloud computing technologies created a unique Alibaba business operating system for this digital era.\n",
      "We believe by leveraging our technology, experience and resources, we can deliver digital transformation for all our clients in the areas of retail, marketing, finance, logistics, manufacturing and other supporting services within the Alibaba business operating system.\n",
      "We delivered another quarter of strong user growth in both MAUs and annual active consumers, facilitated by our effort to target new consumer groups and penetrate into less developed areas in China.\n",
      "We also see enhanced consumer engagement from our existing users that resulted in robust GMV results in which Tmall continues to expand market leadership in B2C e-commerce and Taobao recorded its third consecutive quarter of strong GMV growth.\n",
      "The increase was mainly driven by the robust revenue growth of our China commerce retail business, the consolidation of Ele.me and the Cainiao Network as well as strong revenue growth of Alibaba Cloud.\n",
      "The increase is mainly due to three reasons: number one, our consolidation of Ele.me and Cainiao Network, resulting in higher cost in logistics and fulfillment; number two, the cost of inventory from our New Retail businesses are included due to gross revenue accounting; number three, greater content investments by our digital media and entertainment businesses.\n",
      "While it will take time for us to see financial return for these strategic investments in core commerce, revenue growth of these four business areas has been robust.\n",
      "And then let's take a look at cloud computing revenue growth, 90% year on-year; continued to be very strong, primarily driven by revenue mix towards high value-added products and services and robust growth of paying customers.\n",
      "As Daniel mentioned, we believe the Recommendation Feeds will be further enhanced by the new Taobao interface that include better discovery experience for consumers, enhanced ability to target, engage, and retain consumer by its merchants, increase monetizable properties for the platform.\n",
      "We will continue to grow new users and serve our customers, consumers and merchants with diversified offerings of goods, services and entertainment across our platform.\n",
      "\n",
      " \n",
      "\n",
      "Impact Score :  1.0\n"
     ]
    }
   ],
   "source": [
    "ExtractKeyPhrases(\"/home/kasun/babaTranscript.txt\")\n",
    "ExtractSummary(\"/home/kasun/babaTranscript.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
